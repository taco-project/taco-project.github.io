<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LATTE: Learning to Think with Vision Specialists">
  <meta name="keywords" content="multi-modal model, agent, synthetic data">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LATTE</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link rel="icon" href="./static/images/latte.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zixianma.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://mnms-project.github.io">
            m&ms
          </a>
          <a class="navbar-item" href="https://github.com/RAIVNLab/CREPE">
            CREPE
          </a>
          <a class="navbar-item" href="https://github.com/RAIVNLab/sugar-crepe">
            SugarCREPE
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
            <h1 class="subtitle is-1 publication-title is-bold">
              <img src="./static/images/latte.png" style="width:1em;vertical-align: middle" alt="Logo">
              <span  style="vertical-align: middle">LATTE </br>LeArning to Think wiTh Vision SpEcialists</span>
            </h1>
            <!-- <h1 class="subtitle is-3 publication-title">
              Learning Multi-modal Action Models </br> with Synthetic Chains-of-Thought-and-Action 
            </h1> -->
             
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zixianma.github.io">Zixian Ma</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jianguoz.github.io">Jianguo Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/zhiwei-jim">Zhiwei Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://jieyuz2.github.io">Jieyu Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.juntaotan.com">Juntao Tan</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://azshue.github.io/">Manli Shu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.niebles.net/">Juan Carlos Niebles</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.shelbyh.ai/">Shelby Heinecke</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://huan-december.github.io/index.html">Huan Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://cmxiong.com/">Caiming Xiong</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ranjaykrishna.com/index.html">Ranjay Krishna</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors mt-5">
            <span class="author-block"><sup>1</sup>University of Washington 
              <a href="https://www.cs.washington.edu/"><img src="static/images/uw-logo.svg" style="height: 25px;"> </a> 
            </span>
            
            
           <span class="author-block"> 
              <sup>2</sup>
              Salesforce AI Research <a href="https://www.salesforceairesearch.com/"><img src="static/images/salesforce-logo.svg" style="height: 25px;"> </a>
              
          </a>
        </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2412.05479"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/SalesforceAIResearch/TACO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- HF Links. -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/Salesforce/taco-models-6764b2ad9ed8cf7fc0946581"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">ðŸ¤—</p>
                  </span>
                  <span>Model Weights</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/collections/Salesforce/cota-datasets-675333e57dd34a4adc5f3ff4"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">ðŸ¤—</p>
                  </span>
                  <span>Datasets</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://taco-interactive.streamlit.app/"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">ðŸ’»</p>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <h2 class=" title is-3">How does it work?</h2> -->
       <img src="./static/images/framework.png" alt="teaser image" class="is-fullwidth">
      <!-- <video controls="" height="100%" poster="./static/videos/video_thumbnail.png">
        <source src="./static/videos/taco_video.mp4#t=7.0" fetchpriority="high" type="video/mp4">
      </video> -->
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p class="subtitle">
           While open-source vision-language models perform well on simple question-answering, they
            still struggle with complex questions that require both perceptual and reasoning capabilities. 
            We propose LATTE, a family of vision-language models that have LeArned to Think wiTh vision spEcialists. By offloading perception to state-of-the-art vision models, our approach enables vision-language models to focus solely on reasoning over high-quality per-ceptual information. To train LATTE, we synthesize and filter a large dataset of 273K multi-modal reasoning traces over perceptual outputs of vision specialists. LATTE trained on this data achieves significant gains over baselines across 6 benchmarks covering both perception and reasoning abilities. Ablation studies reveal that the effectiveness of multi-modal reasoning traces depends on the data sources, formats, and quality of thoughts.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero">
  <div class="container">
    <div class="hero-body">
      <img src="./static/images/teaser.png">
      <p class="subtitle">
        Figure 1. <b>Example outputs of LATTE vs. SoTA multi-modal large language models.</b> Our LATTE model is able to answer challenging
visual questions by reasoning over perceptual information output by vision specialists. It does so by generating a reasoning trace over
vision specialists' outputs and producing a final answer based on its reasoning.
      </p>    
      <!-- <video autoplay muted loop height="100%">
        <source src="./static/videos/mnms_video_hd.mp4"
                type="video/mp4">
      </video> -->
    </div>
  </div>
</section>

<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
    <h1 class="title is-1">
      <span style="vertical-align: middle">Additional Examples</span>
    </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered m-3">
      <div class="column is-full has-text-centered content">
        <!-- <h2 class="title is-3">More Examples</h2> -->
        <!-- <p>We present additional examples of TACO success and failure below:</p> -->
          <div id="results-carousel" class="carousel results-carousel" >
            <div class="box m-5 carousel-box">
              <div class="content has-text-centered carousel-content">
                <img src="static/images/additional_examples.png" />
                <p> Successful examples</p>
              </div>
            </div>
            <div class="box m-5 carousel-box">
              <div class="content has-text-centered carousel-content">
                <img src="static/images/additional_examples_2.png" />
                <p> Successful examples</p>
              </div>
            </div>
            <div class="box m-5 carousel-box">
              <div class="content has-text-centered carousel-content">
                <img src="static/images/failure_examples.png"/>
                <p> Failure examples</p>
              </div>
            </div>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- DATA SECTION -->
 <!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
    <h1 class="title is-1">
      <!-- <img src="static/images/mnms_green_single.png" style="width:1em;vertical-align: middle" alt="Logo" /> -->
      <span style="vertical-align: middle">LATTE Traces</span>
    </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered mt-3">
      <div class="column is-four-fifths has-text-centered content">
        <!-- <img src="static/videos/data_gen.gif"/> -->
        <h2 class="title is-3">LATTE-traces Generation</h2>
        <div class="content has-text-justified">
          <img src="./static/images/data_gen.png" alt="dataset generation method">
      <p class="subtitle has-text-centered">
        Figure 2. <b>Data Generation.</b>
      </p>
      <p class="subtitle has-text-justified">
        We illustrate our model-based data generation (top) and programmatic generation (bottom) pipelines. In
        model-based generation, we take existing image and QA pairs as inputs and prompt a large language model (i.e. GPT-4o) to generate
        either a LATTE-trace or chain-of-thought (CoT) to answer the questions. Then, we verify that the chains lead to correct final answers and
        parse successfully; if not, we convert them into the direct answer (Direct) format with groundtruth answers. 
      </p> 
        <p class="subtitle has-text-justified">
          In programmatic generation,
        we first annotate images with human labelers and models, and then use the dense annotations to fill in manually written templates and
        generate QA and the corresponding LATTE-trace with Python programs.
      </p>

      <h2 class="title is-3 has-text-centered mt-6">Data Distribution</h2>
         
          <div class="content">
            <img src="static/images/cota_cot_dist.png" alt="data distribution"/>
            <p class="subtitle">
              Figure 3. <b>Distribution of data formats and sources.</b> We visualize the frequency of data formats (i.e. LATTE-pos/neg, and CoT-pos/neg,
pos = correct final answers, neg = incorrect) in the original GPT-4-generated data and in our training data (i.e. LATTE-trace, CoT, or
Direct) across all data sources. We also highlight the LATTE-useless (i.e. % of CoT-pos - LATTE-pos > 10 or % of LATTE-neg -
LATTE-pos > 10) vs. LATTE-useful datasets.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- EXPERIMENTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1">
      <!-- <img src="static/images/mnms_green_single.png" style="width:1em;vertical-align: middle" alt="Logo" /> -->
      <span style="vertical-align: middle">Experimental Results</span>
    </h1>
  </div>
</section>
<section class="section">
  <div class="container">
    <div class="columns  is-centered mt-3">
      <div class="column is-four-fifths content">
        
        <!-- <h2 class="title is-3">Overview</h2> -->
        <div class="content">
          <p class="subtitle">
            We perform extensive experiments with small multi-modal models and 9 data recipes on 6 benchmarks to study the effectiveness of LATTE-traces in enabling models to reason with vision specialists on diverse vision-language tasks. 
          </p>
        </div>
      </div>
    </div>


    <div class="columns is-centered mt-3">
      <div class="column  is-four-fifths content">
        <!-- <h2 class="title is-3"></h2> -->
          <div class="content">
            <!-- <img src="static/images/table1.png" alt="data distribution"/>
            <p class="subtitle has-text-centered">
              Table 1. CoTA Inference Before vs. After Fine-tuning
            </p> -->
            <!-- <p class="subtitle">
             While GPT-4o performs well with either a direct answer (Direct) or chain-of-
              thought-and-action (CoTA) prompt, open-source multi-modal models lag behind and fail to generate CoTA with few-shot prompting.
            </p> -->
            <h3 class=""> <b>Takeaway 1: </b>
              LATTE leads to substantial gains compared to vanilla instruction-tuning on both perception and reasoning benchmarks, whereas other distillation baselines result in smaller gains or even degradation on some perception tasks. 
            <img src="static/images/latte_vs_distillation.png" alt="latte vs vanilla it results"/>
            <p class="subtitle has-text-centered">
              Table 1. <b>LATTE vs. Baselines on Perception and Reasoning Benchmarks.</b> 
            </p>
            <img src="static/images/scaling_results.png" alt="results of best cota data recipe"/>
            <p class="subtitle has-text-centered">
              Figure 1. <b>Performance of LATTE vs. Baselines across Training Data Scales. </b>
            </p>
                <h3 class=""> <b>Takeaway 2: </b>
                Our method beats the vanilla instruction-tuning baseline on average across all benchmarks regardless of the base model and checkpoint, with significant gains of 10-16% on MMVet.
               </h3>
          </div>
      </div>
    </div>

    
    <div class="columns is-centered mt-3">
      <div class="column is-four-fifths  content">
          <div class="content">
            <img src="static/images/latte_vs_vanilla_it.png" alt="results of best cota data recipe"/>
            <p class="subtitle has-text-centered">
              Table 2. <b>LATTE vs. Vanilla IT with Different Models.</b>
            </p>
           <img src="static/images/error_analysis.png" alt="data distribution"/>
            <p class="subtitle has-text-centered">
              Figure 2. <b>Qualitative analysis.</b> Example outputs of VPD, LLaVA-CoT vs. LATTE on BLINK.
            </p>
              <h3 class=""> <b>Takeaway 3: </b> LATTE performs better in fine-grained perception tasks such as the counting questions in BLINK, while VPD and LLaVA-CoT tend to hallucinate and make perceptual errors.
          </div>
      </div>
    </div>

    <!-- <div class="columns is-centered mt-3">
      <div class="column is-four-fifths  content">
          <div class="content">
            <img src="static/images/table3.png" alt="model-generated data ablations"/>
            <p class="subtitle has-text-centered">
              Table 3. Model-generated Data Ablations
            </p>
              <h3 class=""> <b>Takeaway 3: </b>
                Quality matters more than quantity: the smallest dataset with only
                CoTA examples results in better average performance and higher gains compared to larger datasets with a mix of CoTA, CoT and/or Direct
                examples; and filtering out Action-useless datasets also leads to performance gains.
          </div>
      </div>
    </div>
    <div class="columns is-centered mt-3">
      <div class="column is-four-fifths  content">
          <div class="content">
            <img src="static/images/table4.png" alt="model + program data mixtures"/>
            <p class="subtitle has-text-centered">
              Table 4. Model-generated + Program-generated CoTA Mixtures
            </p>
              <h3 class=""> <b>Takeaway 4: </b>
                Adding programmatically generated data can lead to further gains on some benchmarks but brings no additional gains to the average performance.
          </div>
      </div>
    </div> -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container content">
    <h2 class="title">Citation</h2>
    <pre><code>@misc{ma2024tacolearningmultimodalaction,
      title={TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action}, 
      author={Zixian Ma and Jianguo Zhang and Zhiwei Liu and Jieyu Zhang and Juntao Tan and Manli Shu and Juan Carlos Niebles and Shelby Heinecke and Huan Wang and Caiming Xiong and Ranjay Krishna and Silvio Savarese},
      year={2024},
      eprint={2412.05479},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.05479}, 
}</code></pre>
  </div>
</section>

<footer class="footer">
  <!-- <div class="container"> -->
  <div class="content has-text-centered">
  </div>
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
  <!-- </div> -->
</footer>

</body>
</html>
